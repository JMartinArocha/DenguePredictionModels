{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación de las dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade pip\n",
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de utilidades personales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # Used for making HTTP requests to fetch online resources.\n",
    "\n",
    "# URL of the raw file in the GitHub Gist\n",
    "url = 'https://gist.githubusercontent.com/JMartinArocha/79e6f5c94ab6a8d3f0b2f57296395e76/raw/3d60d10fb336eb870cb03535929502bc8234abc9/ml_utilities.py'\n",
    "\n",
    "# Fetching the content of the file\n",
    "r = requests.get(url)\n",
    "\n",
    "# Writing the content to a local file to ensure the utility script is available for import\n",
    "with open('ml_utilities.py', 'w') as f:\n",
    "    f.write(r.text)\n",
    "\n",
    "# Importing the ml_utilities script after downloading it\n",
    "import ml_utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# just for the sake of this blog post!\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usar el github para la importacion del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs of the data hosted on GitHub\n",
    "url_features_train = 'https://raw.githubusercontent.com/JMartinArocha/MasterBigData/main/Dengue/data/dengue_features_train.csv'\n",
    "url_labels_train = 'https://raw.githubusercontent.com/JMartinArocha/MasterBigData/main/Dengue/data/dengue_labels_train.csv'\n",
    "url_features_test = 'https://raw.githubusercontent.com/JMartinArocha/MasterBigData/main/Dengue/data/dengue_features_test.csv'\n",
    "\n",
    "# Load the data directly from GitHub\n",
    "train_features = pd.read_csv(url_features_train, index_col=[0,1,2])\n",
    "train_labels = pd.read_csv(url_labels_train, index_col=[0,1,2])\n",
    "\n",
    "# Separate data for San Juan\n",
    "sj_train_features = train_features.loc['sj']\n",
    "sj_train_labels = train_labels.loc['sj']\n",
    "\n",
    "# Separate data for Iquitos\n",
    "iq_train_features = train_features.loc['iq']\n",
    "iq_train_labels = train_labels.loc['iq']\n",
    "\n",
    "# Initial inspection of the datasets using custom utility functions\n",
    "ml_utilities.df_look(sj_train_features)\n",
    "ml_utilities.df_look(train_labels)\n",
    "ml_utilities.df_look(iq_train_features)\n",
    "ml_utilities.df_look(iq_train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with forward fill method to ensure continuity and avoid NaN values that could affect the analysis\n",
    "sj_train_features.fillna(method='ffill', inplace=True)\n",
    "iq_train_features.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Merge the total cases into the feature dataset for both San Juan and Iquitos to facilitate direct analysis and modeling\n",
    "sj_train_features['total_cases'] = sj_train_labels.total_cases\n",
    "iq_train_features['total_cases'] = iq_train_labels.total_cases\n",
    "\n",
    "# A second fillna call seems redundant as it was already done previously. Ensure no missing values remain before normalization.\n",
    "# It might be an oversight, or meant as a precautionary step; however, it's good practice to verify the necessity of such steps.\n",
    "sj_train_features.fillna(method='ffill', inplace=True)\n",
    "iq_train_features.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Normalize the datasets using a custom utility function. This standardizes the scale of the features, improving model performance.\n",
    "sj_train_features = ml_utilities.normalize_dataset(sj_train_features)\n",
    "iq_train_features = ml_utilities.normalize_dataset(iq_train_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodos gráficos para la seleccion de caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graphical Methods for Feature Selection\n",
    "\n",
    "# Compute the correlations after dropping non-numeric columns like 'week_start_date' that don't contribute to correlation analysis\n",
    "sj_correlations = sj_train_features.drop(columns=\"week_start_date\").corr()\n",
    "iq_correlations = iq_train_features.drop(columns=\"week_start_date\").corr()\n",
    "\n",
    "# Plot San Juan Variable Correlations using a heatmap\n",
    "sj_corr_heat = sns.heatmap(sj_correlations)\n",
    "plt.title('San Juan Variable Correlations')  # Setting the title for the heatmap\n",
    "plt.show()  # Display the heatmap\n",
    "\n",
    "# Plot Iquitos Variable Correlations in a similar manner\n",
    "iq_corr_heat = sns.heatmap(iq_correlations)\n",
    "plt.title('Iquitos Variable Correlations')  # Setting the title for the heatmap\n",
    "plt.show()  # Display the heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For San Juan: Ranking features based on their correlation with total dengue cases\n",
    "(sj_correlations\n",
    "     .total_cases\n",
    "     .drop('total_cases')  # Exclude self-correlation\n",
    "     .sort_values(ascending=False)  # Sort features by correlation strength\n",
    "     .plot\n",
    "     .barh())  # Generate a horizontal bar chart\n",
    "plt.show()  # Display the chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Iquitos: Repeat the process to analyze and visualize feature correlations\n",
    "(iq_correlations\n",
    "     .total_cases\n",
    "     .drop('total_cases')  # Exclude self-correlation\n",
    "     .sort_values(ascending=False)  # Sort features by correlation strength\n",
    "     .plot\n",
    "     .barh())  # Generate a horizontal bar chart\n",
    "plt.show()  # Display the chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the datasets, excluding 'week_start_date' as it's non-numeric and does not contribute to pair plots\n",
    "ml_utilities.generate_pairwise_pairplots(sj_train_features.drop(columns='week_start_date'), hue='total_cases')\n",
    "ml_utilities.generate_pairwise_pairplots(iq_train_features.drop(columns='week_start_date'), hue='total_cases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos no gráficos para la seleccion de caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Assuming df is your DataFrame with San Juan training features\n",
    "df = sj_train_features\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df.drop(columns=['total_cases','week_start_date'])  # Features, excluding 'week_start_date' as it's non-numeric\n",
    "y = df['total_cases']  # Target variable: total cases of dengue\n",
    "\n",
    "# Initialize SelectKBest with f_classif, the ANOVA F-value function, choosing top 'k' features\n",
    "selector = SelectKBest(f_classif, k=5)  # 'k=5' can be adjusted based on desired number of features\n",
    "\n",
    "# Fit the model and transform the dataset to select the top 'k' features\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Get the names of the selected features\n",
    "KBest_selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "# Print the names of the selected features\n",
    "print(KBest_selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split de datos Train, Test y Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming sj_train_features is your DataFrame with selected San Juan training features\n",
    "df = sj_train_features\n",
    "\n",
    "# Selecting the features for modeling\n",
    "features = ['reanalysis_specific_humidity_g_per_kg', \n",
    "            'reanalysis_dew_point_temp_k', \n",
    "            'station_avg_temp_c', \n",
    "            'station_min_temp_c',\n",
    "            'total_cases']\n",
    "df = df[features]\n",
    "\n",
    "# Separating the dataset into features (X) and target variable (y)\n",
    "X = df.drop(columns=['total_cases'])  # Exclude the target variable from features\n",
    "y = df['total_cases']  # Target variable\n",
    "\n",
    "# Splitting the dataset into training and testing sets, with a test size of 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further splitting the training set into training and validation sets, with validation set being 25% of the training set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# The result is three sets for training, validation, and testing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento - RandomForestRegressor, Cross validation, GridSearch y RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Hyperparameter space definition for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider at every split\n",
    "    'max_depth': [4, 6, 8]  # Maximum number of levels in tree\n",
    "}\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "model_RandomForestRegressor_GS = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "model_RandomForestRegressor_GS.fit(X_train, y_train)  # Training with GridSearchCV\n",
    "best_hyperparameters_GS = model_RandomForestRegressor_GS.best_params_  # Best parameters found by GridSearchCV\n",
    "\n",
    "# Training the model with the best hyperparameters found by GridSearchCV\n",
    "model_RandomForestRegressor_GS = RandomForestRegressor(**best_hyperparameters_GS)\n",
    "model_RandomForestRegressor_GS.fit(X_train, y_train)\n",
    "\n",
    "# Feature names excluding the target variable\n",
    "feature_names = X_train.columns.tolist()\n",
    "# Feature importance visualization for the model trained with GridSearchCV\n",
    "importances_GS = model_RandomForestRegressor_GS.feature_importances_\n",
    "ml_utilities.plot_feature_importance(importances_GS, feature_names, 'Feature Importance of Random Forest Regressor (GridSearchCV)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter space definition for RandomizedSearchCV is the same as for GridSearchCV\n",
    "param_distributions = param_grid\n",
    "\n",
    "# RandomizedSearchCV for hyperparameter tuning\n",
    "model_RandomForestRegressor_RS = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_iter=27, cv=5)\n",
    "model_RandomForestRegressor_RS.fit(X_train, y_train)  # Training with RandomizedSearchCV\n",
    "best_hyperparameters_RS = model_RandomForestRegressor_RS.best_params_  # Best parameters found by RandomizedSearchCV\n",
    "\n",
    "# Training the model with the best hyperparameters found by RandomizedSearchCV\n",
    "model_RandomForestRegressor_RS = RandomForestRegressor(**best_hyperparameters_RS)\n",
    "model_RandomForestRegressor_RS.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance visualization for the model trained with RandomizedSearchCV\n",
    "importances_RS = model_RandomForestRegressor_RS.feature_importances_\n",
    "indices_RS = np.argsort(importances_RS)[::-1]\n",
    "sorted_names_RS = [X.columns[i] for i in indices_RS]\n",
    "\n",
    "# Feature names excluding the target variable\n",
    "feature_names = X_train.columns.tolist()\n",
    "ml_utilities.plot_feature_importance(importances_RS, feature_names, 'Feature Importance of Random Forest Regressor (RandomizedSearchCV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento - DecisionTreeRegressor, GridSearch y RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cambiar a DecisionTreeRegressor\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Ajustar el espacio de hiperparámetros para DecisionTreeRegressor\n",
    "param_grid = {\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [4, 6, 8, None]  # Añadiendo la opción de no limitar la profundidad\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda en cuadrícula para la sintonización de hiperparámetros\n",
    "model_DecisionTreeRegressor_GS = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "model_DecisionTreeRegressor_GS.fit(X_train, y_train)\n",
    "\n",
    "# Obtener y mostrar los mejores hiperparámetros de la búsqueda en cuadrícula\n",
    "best_hyperparameters = model_DecisionTreeRegressor_GS.best_params_\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters)\n",
    "\n",
    "# Realizar predicciones con el modelo entrenado\n",
    "y_pred = model_DecisionTreeRegressor_GS.predict(X_test)\n",
    "\n",
    "# Feature names excluding the target variable\n",
    "feature_names = X_train.columns.tolist()\n",
    "# Feature importance visualization for the model trained with GridSearchCV\n",
    "importances_GS = model_DecisionTreeRegressor_GS.best_estimator_.feature_importances_\n",
    "ml_utilities.plot_feature_importance(importances_GS, feature_names, 'Feature Importance of Decision Tree Regressor (GridSearchCV)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Adjusting the hyperparameter space for a more flexible search\n",
    "param_distributions = {\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'max_depth': sp_randint(3, 20),\n",
    "    'min_samples_split': sp_randint(2, 11),\n",
    "    'min_samples_leaf': sp_randint(1, 11)\n",
    "}\n",
    "\n",
    "# Randomized search for the best hyperparameters\n",
    "model_DecisionTreeRegressor_RS = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_iter=100, cv=5, random_state=42)\n",
    "model_DecisionTreeRegressor_RS.fit(X_train, y_train)\n",
    "# Extract and display the best hyperparameters\n",
    "best_hyperparameters_RS = model_DecisionTreeRegressor_RS.best_params_\n",
    "print(\"Best Hyperparameters from RandomizedSearchCV:\")\n",
    "print(best_hyperparameters_RS)\n",
    "\n",
    "# Realizar predicciones con el modelo entrenado\n",
    "y_pred = model_DecisionTreeRegressor_RS.predict(X_test)\n",
    "\n",
    "# Dado que DecisionTreeRegressor no tiene feature_importances_ de la misma manera que los modelos de ensemble,\n",
    "# la visualización de la importancia de características se hace directamente desde el modelo entrenado.\n",
    "importances = model_DecisionTreeRegressor_RS.best_estimator_.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "sorted_names = [X_train.columns[i] for i in indices]\n",
    "\n",
    "# Feature names excluding the target variable\n",
    "feature_names = X_train.columns.tolist()\n",
    "# Feature importance visualization for the model trained with GridSearchCV\n",
    "importances_RS = model_DecisionTreeRegressor_RS.best_estimator_.feature_importances_\n",
    "ml_utilities.plot_feature_importance(importances_RS, feature_names, 'Feature Importance of Decision Tree Regressor (RandomizedSearchCV)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento - GradientBoostingRegressor, GridSearch y RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the GradientBoostingRegressor model\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Define the hyperparameter space for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "model_GradientBoostingRegressor_GS = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "model_GradientBoostingRegressor_GS.fit(X_train, y_train)\n",
    "\n",
    "# Display the best hyperparameters found by GridSearchCV\n",
    "best_hyperparameters = model_GradientBoostingRegressor_GS.best_params_\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "\n",
    "# Predict on the test set with the tuned model\n",
    "y_pred = model_GradientBoostingRegressor_GS.predict(X_test)\n",
    "\n",
    "\n",
    "# Feature names excluding the target variable\n",
    "feature_names = X_train.columns.tolist()\n",
    "# Feature importance visualization for the model trained with GridSearchCV\n",
    "importances_GS = model_GradientBoostingRegressor_GS.best_estimator_.feature_importances_\n",
    "ml_utilities.plot_feature_importance(importances_GS, feature_names, 'Feature Importance of Gradient Boosting Regressor (GridSearchCV)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint, uniform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize GradientBoostingRegressor model\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Define hyperparameter distributions rather than a fixed hyperparameter grid\n",
    "param_distributions = {\n",
    "    'n_estimators': sp_randint(100, 400),  # Uniform distribution between 100 and 400\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': sp_randint(3, 10),  # Tree depths between 3 and 10\n",
    "    'learning_rate': uniform(0.01, 0.2)  # Continuous values between 0.01 and 0.2\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning with RandomizedSearchCV\n",
    "model_GradientBoostingRegressor_RS = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_iter=100, cv=5, random_state=42)\n",
    "model_GradientBoostingRegressor_RS.fit(X_train, y_train)\n",
    "\n",
    "# Display the best hyperparameters found by RandomizedSearchCV\n",
    "best_hyperparameters = model_GradientBoostingRegressor_RS.best_params_\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "\n",
    "# Predict on the test set with the optimized model\n",
    "y_pred = model_GradientBoostingRegressor_RS.predict(X_test)\n",
    "\n",
    "\n",
    "# Feature names excluding the target variable\n",
    "feature_names = X_train.columns.tolist()\n",
    "# Feature importance visualization for the model trained with GridSearchCV\n",
    "importances_RS = model_GradientBoostingRegressor_RS.best_estimator_.feature_importances_\n",
    "ml_utilities.plot_feature_importance(importances_RS, feature_names, 'Feature Importance of Gradient Boosting Regressor (RandomizedSearchCV)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso de gráficos para obtener comparativas en el entrenamiento y ayudar a entender la presicion de los resutados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dictionary of models for comparison\n",
    "models = {\n",
    "    'Random Forest Regresor - GridSearch': model_RandomForestRegressor_GS,\n",
    "    'Random Forest Regresor - RandomSearch': model_RandomForestRegressor_RS,\n",
    "    'GradientBoostingRegressor - GridSearch': model_GradientBoostingRegressor_GS,\n",
    "    'GradientBoostingRegressor - RandomSearch': model_GradientBoostingRegressor_RS,\n",
    "    'DecisionTreeRegressor - GridSearch': model_DecisionTreeRegressor_GS,\n",
    "    'DecisionTreeRegressor - RandomSearch': model_DecisionTreeRegressor_RS,\n",
    "}\n",
    "\n",
    "models_metrics = {}\n",
    "# Generate and collect metrics for each model\n",
    "for name, model in models.items():\n",
    "    metrics = ml_utilities.generate_regresion_evaluation_report(model, X, y, cv=5)\n",
    "    models_metrics[name] = metrics\n",
    "\n",
    "# Print collected metrics for comparison\n",
    "for model_name, metrics in models_metrics.items():\n",
    "    print(f\"Metrics for {model_name}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "    print()  # Space for readability\n",
    "\n",
    "# Assuming ml_utilities.plot_all_metrics_comparisons is implemented as shown\n",
    "ml_utilities.plot_all_metrics_comparisons(models_metrics, scale_factor=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_vs_actual_(X_train, X_test, y_train, y_test, model):\n",
    "    \"\"\"\n",
    "    Visualiza las predicciones frente a los casos reales de dengue utilizando el modelo entrenado.\n",
    "    Mejorado para legibilidad en gráficos grandes o complejos.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtener predicciones\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    \n",
    "    # Crear DataFrames para la visualización\n",
    "    df_train_pred = pd.DataFrame({'Actual': y_train, 'Predicted': train_predictions}).sample(n=100)  # Ajusta n según sea necesario\n",
    "    df_test_pred = pd.DataFrame({'Actual': y_test, 'Predicted': test_predictions}).sample(n=50)  # Ajusta n según sea necesario\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 12))  # Ajusta el tamaño según sea necesario\n",
    "\n",
    "    # Gráfico para el conjunto de entrenamiento\n",
    "    df_train_pred.plot(ax=axes[0], marker='o', linestyle='-', markersize=5, alpha=0.6)\n",
    "    axes[0].set_title(\"Training Set: Predicted vs Actual Cases\")\n",
    "    axes[0].legend([\"Actual\", \"Predicted\"])\n",
    "    \n",
    "    # Gráfico para el conjunto de prueba\n",
    "    df_test_pred.plot(ax=axes[1], marker='o', linestyle='-', markersize=5, alpha=0.6)\n",
    "    axes[1].set_title(\"Test Set: Predicted vs Actual Cases\")\n",
    "    axes[1].legend([\"Actual\", \"Predicted\"])\n",
    "\n",
    "    plt.suptitle(\"Dengue Predicted Cases vs. Actual Cases\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "print(f'{type(model_RandomForestRegressor_GS).__name__} Grid Search')\n",
    "plot_predictions_vs_actual_(X_train, X_test, y_train, y_test, model_RandomForestRegressor_GS)\n",
    "\n",
    "print(f'{type(model_RandomForestRegressor_RS).__name__} Random Search')\n",
    "plot_predictions_vs_actual_(X_train, X_test, y_train, y_test, model_RandomForestRegressor_RS)\n",
    "\n",
    "print(f'{type(model_GradientBoostingRegressor_GS).__name__} Grid Search')\n",
    "plot_predictions_vs_actual_(X_train, X_test, y_train, y_test, model_GradientBoostingRegressor_GS)\n",
    "\n",
    "print(f'{type(model_GradientBoostingRegressor_RS).__name__} Random Search')\n",
    "plot_predictions_vs_actual_(X_train, X_test, y_train, y_test, model_GradientBoostingRegressor_RS)\n",
    "\n",
    "print(f'{type(model_DecisionTreeRegressor_GS).__name__} Grid Search')\n",
    "plot_predictions_vs_actual_(X_train, X_test, y_train, y_test, model_DecisionTreeRegressor_GS)\n",
    "\n",
    "print(f'{type(model_DecisionTreeRegressor_RS).__name__} Random Search')\n",
    "plot_predictions_vs_actual_(X_train, X_test, y_train, y_test, model_DecisionTreeRegressor_RS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion VS Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_vs_validation(X_train, X_val, y_train, y_val, model):\n",
    "    \"\"\"\n",
    "    Visualiza las predicciones frente a los casos reales de dengue en el conjunto de validación utilizando el modelo entrenado.\n",
    "\n",
    "    Parámetros:\n",
    "    - X_train: Características del conjunto de entrenamiento.\n",
    "    - X_val: Características del conjunto de validación.\n",
    "    - y_train: Etiquetas del conjunto de entrenamiento.\n",
    "    - y_val: Etiquetas del conjunto de validación.\n",
    "    - model: Modelo entrenado.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtener predicciones para el conjunto de entrenamiento y validación\n",
    "    train_predictions = model.predict(X_train)\n",
    "    val_predictions = model.predict(X_val)\n",
    "    \n",
    "    # Crear DataFrames para la visualización\n",
    "    df_train_pred = pd.DataFrame({'Actual': y_train, 'Predicted': train_predictions}).sample(n=100)  # Ajusta según necesidad\n",
    "    df_val_pred = pd.DataFrame({'Actual': y_val, 'Predicted': val_predictions}).sample(n=50)  # Ajusta según necesidad\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 12))\n",
    "\n",
    "    # Gráfico para el conjunto de entrenamiento\n",
    "    df_train_pred.plot(ax=axes[0], marker='o', linestyle='-', markersize=5, alpha=0.6)\n",
    "    axes[0].set_title(\"Training Set: Predicted vs Actual Cases\")\n",
    "    axes[0].legend([\"Actual\", \"Predicted\"])\n",
    "    \n",
    "    # Gráfico para el conjunto de validación\n",
    "    df_val_pred.plot(ax=axes[1], marker='o', linestyle='-', markersize=5, alpha=0.6)\n",
    "    axes[1].set_title(\"Validation Set: Predicted vs Actual Cases\")\n",
    "    axes[1].legend([\"Actual\", \"Predicted\"])\n",
    "\n",
    "    plt.suptitle(\"Dengue Predicted Cases: Training vs. Validation Sets\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "print(f'{type(model_RandomForestRegressor_GS).__name__} Grid Search')\n",
    "plot_predictions_vs_validation(X_train, X_val, y_train, y_val, model_RandomForestRegressor_GS)\n",
    "print(f'{type(model_RandomForestRegressor_RS).__name__} Random Search')\n",
    "plot_predictions_vs_validation(X_train, X_val, y_train, y_val, model_RandomForestRegressor_RS)\n",
    "print(f'{type(model_GradientBoostingRegressor_GS).__name__} Grid Search')\n",
    "plot_predictions_vs_validation(X_train, X_val, y_train, y_val, model_GradientBoostingRegressor_GS)\n",
    "print(f'{type(model_GradientBoostingRegressor_RS).__name__} Random Search')\n",
    "plot_predictions_vs_validation(X_train, X_val, y_train, y_val, model_GradientBoostingRegressor_RS)\n",
    "print(f'{type(model_DecisionTreeRegressor_GS).__name__} Grid Search')\n",
    "plot_predictions_vs_validation(X_train, X_val, y_train, y_val, model_DecisionTreeRegressor_GS)\n",
    "print(f'{type(model_DecisionTreeRegressor_RS).__name__} Random Search')\n",
    "plot_predictions_vs_validation(X_train, X_val, y_train, y_val, model_DecisionTreeRegressor_RS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
